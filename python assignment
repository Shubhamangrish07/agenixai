Scenario 1: Extract Data from API and Load it into PostgreSQL

!pip install requests psycopg2
import requests
import psycopg2
from datetime import datetime
DB_CONNECTION_PARAMS = {
    'dbname': 'your_db_name',
    'user': 'your_db_user',
    'password': 'your_db_password',
    'host': 'localhost',
    'port': '5432'
}

class NewsAPIExtractor:
    def __init__(self, api_key, db_connection_params):
        self.api_key = api_key
        self.db_connection_params = db_connection_params
        self.base_url = "https://newsapi.org/v2/top-headlines"
    
    def fetch_articles(self, country='us'):
        """Fetch top news articles from the API"""
        params = {
            'country': country,
            'apiKey': self.api_key
        }
        response = requests.get(self.base_url, params=params)
        
        if response.status_code == 200:
            return response.json()['articles']
        else:
            raise Exception(f"Failed to fetch data. HTTP Status Code: {response.status_code}")
    
    def store_articles(self, articles):
        """Store the fetched articles into PostgreSQL database"""
        conn = psycopg2.connect(**self.db_connection_params)
        cursor = conn.cursor()

        insert_query = """
            INSERT INTO news_articles (source_name, author, title, description, url, published_at, content)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
        """
        
        for article in articles:
            source_name = article['source']['name']
            author = article.get('author', 'N/A')
            title = article['title']
            description = article.get('description', 'N/A')
            url = article['url']
            published_at = datetime.strptime(article['publishedAt'], '%Y-%m-%dT%H:%M:%S%z')
            content = article.get('content', 'N/A')

            cursor.execute(insert_query, (source_name, author, title, description, url, published_at, content))

        conn.commit()
        cursor.close()
        conn.close()
        print(f"{len(articles)} articles inserted into the database.")
    
    def extract_and_store(self, country='us'):
        """Fetch and store articles in the database"""
        try:
            articles = self.fetch_articles(country)
            self.store_articles(articles)
        except Exception as e:
            print(f"Error: {e}")
API_KEY = 'your_news_api_key'

extractor = NewsAPIExtractor(API_KEY, DB_CONNECTION_PARAMS)
extractor.extract_and_store(country='us')


Scenario 2: Webserver Log Parser and Reporting Metrics


!pip install psycopg2


import re
import psycopg2
from collections import defaultdict
from datetime import datetime
Step 3: Setup Database Connection

DB_CONNECTION_PARAMS = {
    'dbname': 'your_db_name',
    'user': 'your_db_user',
    'password': 'your_db_password',
    'host': 'localhost',
    'port': '5432'
}


class LogParser:
    def __init__(self, log_file, db_connection_params):
        self.log_file = log_file
        self.db_connection_params = db_connection_params
        self.status_codes = defaultdict(int)
        self.page_requests = defaultdict(int)
        self.ip_addresses = defaultdict(int)
    
    def parse_log(self):
        """Parse the Apache log file and extract relevant information."""
        log_pattern = r'(\S+) - - \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\d+) ".*?" "(.*?)"'
        
        with open(self.log_file, 'r') as file:
            for line in file:
                match = re.match(log_pattern, line)
                if match:
                    ip, timestamp, method, url, protocol, status_code, _, user_agent = match.groups()
                    timestamp = datetime.strptime(timestamp, '%d/%b/%Y:%H:%M:%S %z')
                    self.store_log_data(ip, timestamp, method, url, int(status_code), user_agent)
                    self.status_codes[status_code] += 1
                    self.page_requests[url] += 1
                    self.ip_addresses[ip] += 1
    
    def store_log_data(self, ip, timestamp, method, url, status_code, user_agent):
        """Store log data in the PostgreSQL database."""
        conn = psycopg2.connect(**self.db_connection_params)
        cursor = conn.cursor()

        insert_query = """
            INSERT INTO web_logs (ip_address, timestamp, http_method, url_path, status_code, user_agent)
            VALUES (%s, %s, %s, %s, %s, %s)
        """
        cursor.execute(insert_query, (ip, timestamp, method, url, status_code, user_agent))

        conn.commit()
        cursor.close()
        conn.close()

    def get_metrics(self):
        """Return metrics: total requests, unique IPs, top IPs, top URLs."""
        total_requests = sum(self.status_codes.values())
        top_ips = sorted(self.ip_addresses.items(), key=lambda x: x[1], reverse=True)[:10]
        top_urls = sorted(self.page_requests.items(), key=lambda x: x[1], reverse=True)[:10]
        
        return total_requests, len(self.ip_addresses), top_ips, top_urls

log_file = 'access.log'  # Path to the Apache log file
parser = LogParser(log_file, DB_CONNECTION_PARAMS)
parser.parse_log()

total_requests, unique_ips, top_ips, top_urls = parser.get_metrics()

print(f"Total Requests: {total_requests}")
print(f"Unique IP Addresses: {unique_ips}")
print("Top 10 IP Addresses:")
for ip, count in top_ips:
    print(f"  {ip}: {count}")

print("Top 10 Requested URL Paths:")
for url, count in top_urls:
    print(f"  {url}: {count}")

SELECT COUNT(*) FROM web_logs;


SELECT COUNT(DISTINCT ip_address) FROM web_logs;


SELECT ip_address, COUNT(*) AS request_count
FROM web_logs
GROUP BY ip_address
ORDER BY request_count DESC
LIMIT 10;


SELECT url_path, COUNT(*) AS request_count
FROM web_logs
GROUP BY url_path
ORDER BY request_count DESC
LIMIT 10;


SELECT EXTRACT(HOUR FROM timestamp) AS hour, COUNT(*) AS request_count
FROM web_logs
GROUP BY hour
ORDER BY request_count DESC
LIMIT 1;
Summary
Scenario 1: Extract data from the News API and store it in PostgreSQL using a Python class. You will need to provide your API key for NewsAPI and set up the PostgreSQL connection parameters accordingly.

Scenario 2: Parse Apache web server logs, extract data such as IP address, timestamp, HTTP method, status code, and user agent, and store it in a PostgreSQL database. Then, you can calculate useful metrics such as the number of unique IPs, top IPs, and the busiest hour of the day based on the logs.

Each step is modular, and you can run each code cell independently to analyze or extend the functionality as needed.


